{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgqXOfV6i5EB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "efbbbfa9-cadb-42df-dbe3-14d32fa54ffb"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "\n",
        "def filter_insignificant(chunk, tag_prefix):\n",
        "    good = []\n",
        "\n",
        "    for word, tag in chunk:\n",
        "        ok = True\n",
        "        for prefix in tag_prefix:\n",
        "            if tag.startswith(prefix):\n",
        "                ok = False\n",
        "                break\n",
        "\n",
        "        if ok:\n",
        "            good.append(word)\n",
        "\n",
        "    return good\n",
        "\n",
        "\n",
        "def get_wordnet_pos(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # lower text\n",
        "    text = text.lower()\n",
        "    # tokenize text and remove puncutation\n",
        "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
        "    # remove words that contain numbers\n",
        "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
        "    # remove stop words\n",
        "    stop = stopwords.words('english')\n",
        "    text = [x for x in text if x not in stop]\n",
        "    # remove empty tokens\n",
        "    text = [t for t in text if len(t) > 0]\n",
        "    # pos tag text\n",
        "    pos_tags = pos_tag(text)\n",
        "    pos_tags = filter_insignificant(pos_tags,'N')\n",
        "    # lemmatize text\n",
        "    text = [WordNetLemmatizer().lemmatize(t) for t in pos_tags]\n",
        "    # remove words with only one letter\n",
        "    text = [t for t in text if len(t) > 1]\n",
        "    # join all\n",
        "    text = \" \".join(text)\n",
        "    # print(\"cleaning complete for\",text)\n",
        "    return (text)\n",
        "\n",
        "\n",
        "# wordcloud function\n",
        "def show_wordcloud(data, title = None):\n",
        "    wordcloud = WordCloud(\n",
        "        background_color = 'white',\n",
        "        max_words = 200,\n",
        "        max_font_size = 40,\n",
        "        scale = 3,\n",
        "        random_state = 42\n",
        "    ).generate(str(data))\n",
        "\n",
        "    fig = plt.figure(1, figsize = (20, 20))\n",
        "    plt.axis('off')\n",
        "    if title:\n",
        "        fig.suptitle(title, fontsize = 20)\n",
        "        fig.subplots_adjust(top = 2.3)\n",
        "\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.show()\n",
        "\n",
        "def countVectorizer(reviews_df):\n",
        "    bow_vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'),\n",
        "                                     ngram_range=(1, 2))\n",
        "    bow_result = bow_vectorizer.fit_transform(reviews_df[\"review_clean\"]).toarray()\n",
        "    bow_df = pd.DataFrame(bow_result, columns=bow_vectorizer.get_feature_names())\n",
        "    bow_df.index = reviews_df.index\n",
        "    bow_df_final = pd.concat([bow_df, reviews_df[\"review_status\"]], axis=1)\n",
        "    return bow_vectorizer,bow_df_final\n",
        "\n",
        "def tfidfVectorizer(reviews_df):\n",
        "    tfidf_vectorizer = TfidfVectorizer(min_df=10)\n",
        "    tfidf_result = tfidf_vectorizer.fit_transform(reviews_df[\"review_clean\"]).toarray()\n",
        "    tfidf_df = pd.DataFrame(tfidf_result, columns=tfidf_vectorizer.get_feature_names())\n",
        "    tfidf_df.index = reviews_df.index\n",
        "    tfidf_df_final = pd.concat([tfidf_df, reviews_df[\"review_status\"]], axis=1)\n",
        "    return tfidf_vectorizer,tfidf_df_final\n",
        "\n",
        "def model_train(final_df):\n",
        "    # feature selection\n",
        "    label = \"review_status\"\n",
        "    ignore_cols = [label, \"review\", \"review_clean\"]\n",
        "    features = [c for c in final_df.columns if c not in ignore_cols]\n",
        "\n",
        "    # split the data into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(final_df[features], final_df[label], test_size=0.20,\n",
        "                                                        random_state=42)\n",
        "    return X_train, X_test, y_train, y_test\n",
        "    # train a random forest classifier\n",
        "\n",
        "\n",
        "def training_report(rf,X_test,y_test):\n",
        "    print(\"Accuracy Score {} \".format(rf.score(X_test, y_test)))\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(\"Confusion Matrix  {}\".format(confusion_matrix(y_test, y_pred)))\n",
        "    print(\"Classification report {} \".format(classification_report(y_test, y_pred)))\n",
        "    print(\"Accuracy Score {}\".format(accuracy_score(y_test, y_pred)))\n",
        "\n",
        "# read data\n",
        "def main():\n",
        "    reviews_df = pd.read_csv(\"train_Reviews.csv\")\n",
        "    dmap = {\"good\": 1, \"bad\":-1,\"neutral\" : 0}\n",
        "    reviews_df[\"review_status\"] = reviews_df[\"classification\"].replace(dmap)\n",
        "\n",
        "    # select only relevant columns\n",
        "    reviews_df = reviews_df[[\"review\", \"review_status\"]]\n",
        "    reviews_df = reviews_df.sample(frac=0.1, replace=False, random_state=42)\n",
        "    reviews_df[\"review\"] = reviews_df[\"review\"].apply(\n",
        "        lambda x: str(x).replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))\n",
        "    # print wordcloud\n",
        "    # clean text data\n",
        "    reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(str(x)))\n",
        "    show_wordcloud(reviews_df[\"review_clean\"])\n",
        "\n",
        "    bow_vectorizer,bow_df_final = countVectorizer(reviews_df)\n",
        "    X_train, X_test, y_train, y_test = model_train(bow_df_final)\n",
        "    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    print(\"###### Reports for Count Vectorizer #######\")\n",
        "    training_report(rf,X_test,y_test)\n",
        "\n",
        "    tfidf_vectorizer,tfidf_df_final =tfidfVectorizer(reviews_df)\n",
        "    X_train, X_test, y_train, y_test = model_train(tfidf_df_final)\n",
        "    rf2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    rf2.fit(X_train, y_train)\n",
        "    print(\"###### Reports for TFIDF Vectorizer #######\")\n",
        "    training_report(rf2, X_test, y_test)\n",
        "    \n",
        "    \n",
        "    ######Prediction\n",
        "    predict_df = pd.read_csv(\"predict_Reviews.csv\")\n",
        "    dmap = {\"good\": 1, \"bad\": -1, \"neutral\": 0}\n",
        "    predict_df['review_status'] = predict_df['classification'].replace(dmap)\n",
        "    reviews_df = predict_df[[\"review\", \"review_status\"]]\n",
        "    reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(str(x)))\n",
        "    \n",
        "    \n",
        "    ####Count Vect\n",
        "    gen_result = bow_vectorizer.transform(reviews_df[\"review_clean\"]).toarray()\n",
        "    pred_df = pd.DataFrame(gen_result, columns=bow_vectorizer.get_feature_names())\n",
        "\n",
        "    pred_df.index = reviews_df.index\n",
        "    pred_df_final = pd.concat([pred_df, reviews_df[\"review_status\"]], axis=1)\n",
        "\n",
        "    gen_label = \"review_status\"\n",
        "    ignore_cols = [gen_label, \"review\", \"review_clean\"]\n",
        "    pred_features = [c for c in pred_df_final.columns if c not in ignore_cols]\n",
        "    pred_df_final.fillna(0, inplace=True)\n",
        "    pred_x = pred_df_final[pred_features]\n",
        "    pred_y = pred_df_final[gen_label]\n",
        "    print(\"Count Vectorizer accuracy\")\n",
        "    print(rf.score(pred_x, pred_y))\n",
        "    \n",
        "    \n",
        "    ####TFIDF Vect\n",
        "    print(\"TFID vECTORIZER ACCURACY\")\n",
        "    \n",
        "    gen_result = tfidf_vectorizer.transform(reviews_df[\"review_clean\"]).toarray()\n",
        "    pred_df = pd.DataFrame(gen_result, columns=tfidf_vectorizer.get_feature_names())\n",
        "\n",
        "    pred_df.index = reviews_df.index\n",
        "    pred_df_final = pd.concat([pred_df, reviews_df[\"review_status\"]], axis=1)\n",
        "\n",
        "    gen_label = \"review_status\"\n",
        "    ignore_cols = [gen_label, \"review\", \"review_clean\"]\n",
        "    pred_features = [c for c in pred_df_final.columns if c not in ignore_cols]\n",
        "    pred_df_final.fillna(0, inplace=True)\n",
        "    pred_x = pred_df_final[pred_features]\n",
        "    pred_y = pred_df_final[gen_label]\n",
        "\n",
        "    print(rf2.score(pred_x, pred_y))\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNRUdorRjPqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690
        },
        "outputId": "b3926a19-57a7-4462-be79-3075958cfdbe"
      },
      "source": [
        "main()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'classification'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-48a27a435eb3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mreviews_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Hotel_Reviews.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0mdmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"good\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bad\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"neutral\"\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_status\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"classification\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;31m# select only relevant columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'classification'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc4vpcyxtg8W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "9545f569-37fd-4663-d90d-0c79f2b06633"
      },
      "source": [
        "predict_df = pd.read_csv(\"predict_Reviews.csv\")\n",
        "dmap = {\"good\": 1, \"bad\": -1, \"neutral\": 0}\n",
        "predict_df['review_status'] = predict_df['classification'].replace(dmap)\n",
        "reviews_df = predict_df[[\"review\", \"review_status\"]]\n",
        "reviews_df[\"review_clean\"] = reviews_df[\"review\"].apply(lambda x: clean_text(str(x)))\n",
        "\n",
        "gen_result = bow_vectorizer.transform(reviews_df[\"review_clean\"]).toarray()\n",
        "pred_df = pd.DataFrame(gen_result, columns=bow_vectorizer.get_feature_names())\n",
        "\n",
        "pred_df.index = reviews_df.index\n",
        "pred_df_final = pd.concat([pred_df, reviews_df[\"review_status\"]], axis=1)\n",
        "\n",
        "gen_label = \"review_status\"\n",
        "ignore_cols = [gen_label, \"review\", \"review_clean\"]\n",
        "pred_features = [c for c in pred_df_final.columns if c not in ignore_cols]\n",
        "pred_df_final.fillna(0, inplace=True)\n",
        "pred_x = pred_df_final[pred_features]\n",
        "pred_y = pred_df_final[gen_label]\n",
        "print(\"Count Vectorizer accuracy\")\n",
        "print(rf.score(pred_x, pred_y))\n",
        "print(\"TFID vECTORIZER ACCURACY\")\n",
        "print(rf2.score(pred_x,pred_y))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9b27699f64ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_clean\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgen_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review_clean\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mpred_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbow_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'transform'"
          ]
        }
      ]
    }
  ]
}